{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Model Comparision.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytUPpbEtL9Nl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d94c9d2-13f1-499a-9f47-0a69e82aca67"
      },
      "source": [
        "!pip install unidecode contractions --quiet\n",
        "!pip install emot --upgrade --quiet\n",
        "!pip install vaderSentiment --upgrade --quiet\n",
        "!pip install textblob --upgrade --quiet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 235 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 321 kB 51.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 284 kB 66.8 MB/s \n",
            "\u001b[?25h  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 61 kB 19 kB/s \n",
            "\u001b[K     |████████████████████████████████| 125 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 4.8 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGpwN8pLMlm6",
        "outputId": "0460065e-7a02-4ec8-8930-3f77d7c62260"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import emot\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import nltk\n",
        "import spacy\n",
        "import unidecode \n",
        "from bs4 import BeautifulSoup\n",
        "import contractions\n",
        "import pickle\n",
        "from emot.emo_unicode import EMOTICONS_EMO \n",
        "from emot.emo_unicode import EMOJI_UNICODE,UNICODE_EMOJI\n",
        "from sklearn.metrics import f1_score,accuracy_score\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def lower_case(text) :\n",
        "  return text.lower()\n",
        "\n",
        "def strip_html_tags(text) :\n",
        "  \"\"\"remove html tags from text\"\"\"\n",
        "  soup = BeautifulSoup(text,\"html.parser\")\n",
        "  stripped_text = soup.get_text(separator=\" \")\n",
        "  return stripped_text\n",
        " \n",
        " \n",
        "def accented_chars_to_ascii(text) :\n",
        "  \"\"\"Remove accented characters from text\"\"\"\n",
        "  text = unidecode.unidecode(text)\n",
        "  return text\n",
        "  \n",
        "  \n",
        "def expand_contractions(text) :\n",
        "  \"\"\"expand shortend words, e.g. `don't` to `do not` \"\"\"\n",
        "  text = contractions.fix(text)\n",
        "  return text\n",
        "  \n",
        "  \n",
        "def remove_urls(text) :\n",
        "  url_pattern = re.compile(r'https?:\\/\\/\\S+|www\\.\\S+')\n",
        "  return url_pattern.sub(r'',text)\n",
        "  \n",
        "  \n",
        "def remove_twitter_handles(text) :\n",
        "  pattern = re.compile(r'@[^\\s]+')\n",
        "  return pattern.sub(r'',text)\n",
        "  \n",
        "  \n",
        "def convert_emoticons(text) :\n",
        "  for emot in EMOTICONS_EMO:\n",
        "    text = re.sub(u'('+re.escape(emot)+')', \" \" + \"_\".join(EMOTICONS_EMO[emot].replace(\",\",\"\").split())+\" \", text)\n",
        "  return text\n",
        "  \n",
        "  \n",
        "def convert_emojis(text):\n",
        "  for emot in UNICODE_EMOJI :\n",
        "    text = re.sub(r'('+re.escape(emot)+')',\"_\".join(UNICODE_EMOJI[emot].replace(',','').replace(\":\",\"\").split()),text)\n",
        "  return text\n",
        "  \n",
        "  \n",
        "def remove_digts(text) :\n",
        "  return re.sub(r'\\w*\\d\\w*',' ',text)\n",
        "  \n",
        "  \n",
        "def remove_punctuations(text) :\n",
        "  return text.translate(str.maketrans('','',string.punctuation))\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  return \" \".join([word for word in str(text).split() if word not in stop_words])  \n",
        "  \n",
        "  \n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_words(text) :\n",
        "  return \" \".join([stemmer.stem(word) for word in text.split()]) \n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_words(text) :\n",
        "  return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3CRvBwiNCwQ"
      },
      "source": [
        "def testing_model(model,vectorizer,y_train) :\n",
        "  pred = model.predict(vectorizer)\n",
        "  f1 = f1_score(y_train,pred)\n",
        "  acc = accuracy_score(y_train,pred)\n",
        "  print(f\"F1 Score : {f1}\")\n",
        "  print(f\"Acc. : {acc}\")\n",
        "\n",
        "def cleaning_pipeline(x) :\n",
        "  x=x.fillna(\"\")\n",
        "  x = x.apply(lower_case)\n",
        "  x = x.apply(strip_html_tags)\n",
        "  x = x.apply(accented_chars_to_ascii)\n",
        "  x = x.apply(remove_urls)\n",
        "  x = x.apply(remove_twitter_handles)\n",
        "  # x = x.apply(convert_emoticons)\n",
        "  # x = x.apply(convert_emojis)\n",
        "  x = x.apply(remove_digts)\n",
        "  x = x.apply(remove_punctuations)\n",
        "  x = x.apply(remove_stopwords)\n",
        "  x = x.apply(expand_contractions)\n",
        "  # x = x.apply(stem_words)\n",
        "  x = x.apply(lemmatize_words)\n",
        "  return x\n",
        "\n",
        "def testing_model(model,vectorizer,y_train) :\n",
        "  pred = model.predict(vectorizer)\n",
        "  f1 = f1_score(y_train,pred)\n",
        "  acc = accuracy_score(y_train,pred)\n",
        "  print(f\"F1 Score : {f1}\")\n",
        "  print(f\"Acc. : {acc}\")\n",
        "\n",
        "def testing_metrics(y_pred,y_test):\n",
        "  f1 = f1_score(y_test,y_pred)\n",
        "  acc = accuracy_score(y_test,y_pred)\n",
        "  print(f\"F1 Score : {f1}\")\n",
        "  print(f\"Acc. : {acc}\")\n",
        "\n",
        "def testing_pipeline(x,y,model,vectorizer) :\n",
        "  pow = vectorizer.transform(x)\n",
        "  testing_model(model,pow,y)\n",
        "  # return pow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "RG0LEE7xM98J",
        "outputId": "252b6a1f-c27d-4a58-dcbc-6db3844035cb"
      },
      "source": [
        "path = \"/content/drive/MyDrive/DataSet/Sentiment140/sentiment140.csv\"\n",
        "df = pd.read_csv(path,encoding='latin')\n",
        "df.columns = ['sentiment', 'id', 'date', 'flag', 'user', 'text']\n",
        "df = df[['sentiment','text']]\n",
        "df.drop_duplicates(inplace=True)\n",
        "df['sentiment'] = df['sentiment'].apply(lambda x:1 if x == 4 else 0)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kwesidei not the whole crew</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                               text\n",
              "0          0  is upset that he can't update his Facebook by ...\n",
              "1          0  @Kenichan I dived many times for the ball. Man...\n",
              "2          0    my whole body feels itchy and like its on fire \n",
              "3          0  @nationwideclass no, it's not behaving at all....\n",
              "4          0                      @Kwesidei not the whole crew "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvrL48BeNG2A"
      },
      "source": [
        "X,y = df['text'],df['sentiment']\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjXYz0QKNHcE"
      },
      "source": [
        "X_train1,X_test1 = cleaning_pipeline(X_train),cleaning_pipeline(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaoOiSelNLl2"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "bow = CountVectorizer(min_df=5).fit(X_train1)\n",
        "bow1 = CountVectorizer().fit(X_train1)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(min_df=5).fit(X_train1)\n",
        "tfidf1 = TfidfVectorizer().fit(X_train1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQQSQfUa2WwS"
      },
      "source": [
        "import pickle\n",
        "pickle.dump(bow,open('Bow_5.pkl','wb'))\n",
        "pickle.dump(bow1,open('Bow.pkl','wb'))\n",
        "pickle.dump(tfidf,open('Tfidf_5.pkl','wb'))\n",
        "pickle.dump(tfidf1,open('Tfidf.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "YrIJ0iAV4lbl",
        "outputId": "57ca4eac-32e5-4d6f-8f72-ff671d591b2c"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('/content/Bow.pkl')\n",
        "files.download('/content/Bow5.pkl')\n",
        "files.download('/content/Tfidf.pkl')\n",
        "files.download('/content/Tfidf5.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_720594ec-82bc-4bdf-bc51-21973780bf9c\", \"Bow.pkl\", 7559443)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_06f33439-0cc7-4099-8a93-fbb346bf52cb\", \"Bow5.pkl\", 6177310)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_4c9f125d-451f-4e2d-8788-bf8e434e7d60\", \"Tfidf.pkl\", 12814535)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_5a7eb4a8-81de-4c3a-8edc-23564b7c8463\", \"Tfidf5.pkl\", 6864854)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkAwcL-nZtuq"
      },
      "source": [
        "x_train_bow = bow.transform(X_train1)\n",
        "x_train_bow1 = bow1.transform(X_train1)\n",
        "x_train_tfidf = tfidf.transform(X_train1)\n",
        "x_train_tfidf1 = tfidf1.transform(X_train1)\n",
        "x_test_bow = bow.transform(X_test1)\n",
        "x_test_bow1 = bow1.transform(X_test1)\n",
        "x_test_tfidf = tfidf.transform(X_test1)\n",
        "x_test_tfidf1 = tfidf1.transform(X_test1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3e5ElsBQANM"
      },
      "source": [
        "# TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "U6JfdhAtTPHD",
        "outputId": "c924d203-abfa-40d6-ac55-c5c06476f601"
      },
      "source": [
        "X_test.iloc[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'redoinq the myspace '"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdGIftE2N_zx",
        "outputId": "8d77acb1-5eb9-44a1-fcf4-051ae6daf21c"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "pred = np.zeros(len(X_test))\n",
        "for i in tqdm(range(len(pred))):\n",
        "  pred[i] = np.round(TextBlob(X_test.iloc[i]).sentiment.polarity/2+0.5)\n",
        "\n",
        "print(\"\\nTesting TextBlob without Cleaning Data\")\n",
        "testing_metrics(pred,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 316738/316738 [01:42<00:00, 3085.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing TextBlob without cleaning\n",
            "F1 Score : 0.5994065480655877\n",
            "Acc. : 0.6223566480813796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjtKw4wEXCGf",
        "outputId": "87f4bf2f-c163-46c9-b6aa-55149d61e25a"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "pred = np.zeros(len(X_test))\n",
        "for i in tqdm(range(len(pred))):\n",
        "  pred[i] = np.round(TextBlob(X_test1.iloc[i]).sentiment.polarity/2+0.5)\n",
        "\n",
        "print(\"\\nTesting TextBlob with cleaned Data\")\n",
        "testing_metrics(pred,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 316738/316738 [01:17<00:00, 4063.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing TextBlob with cleaned Data\n",
            "F1 Score : 0.5914649431244623\n",
            "Acc. : 0.6192373507441482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TDx73tlXwWk"
      },
      "source": [
        "# Vader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i5NTLS3XoX_",
        "outputId": "dc283c67-2593-46d4-d2c3-bf53908712a5"
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "vader = SentimentIntensityAnalyzer()\n",
        "\n",
        "pred = np.zeros(len(X_test))\n",
        "for i in tqdm(range(len(pred))):\n",
        "  pred[i] = np.round(vader.polarity_scores(X_test.iloc[i])['compound']/2+0.5)\n",
        "\n",
        "print(\"\\nTesting Vader without Cleaning Data\")\n",
        "testing_metrics(pred,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 316738/316738 [00:45<00:00, 6954.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing Vader without Cleaning Data\n",
            "F1 Score : 0.6453152845957371\n",
            "Acc. : 0.6518952572788866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnYWuEwrYCml",
        "outputId": "e2a68043-3e29-46a2-d9b4-34634de9e2a8"
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "vader = SentimentIntensityAnalyzer()\n",
        "\n",
        "pred = np.zeros(len(X_test))\n",
        "for i in tqdm(range(len(pred))):\n",
        "  pred[i] = np.round(vader.polarity_scores(X_test1.iloc[i])['compound']/2+0.5)\n",
        "\n",
        "print(\"\\nTesting Vader with Cleaned Data\")\n",
        "testing_metrics(pred,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 316738/316738 [00:32<00:00, 9722.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing Vader with Cleaned Data\n",
            "F1 Score : 0.6352670078847329\n",
            "Acc. : 0.6362008979029987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PExVWJd1Yo_J"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3DHbzq8Y7-X",
        "outputId": "9f053e9c-3e23-46e5-b5f7-c6782fbf4230"
      },
      "source": [
        "model = pickle.load(open('/content/Logistic_Regression_bow_lem.pkl','rb'))\n",
        "\n",
        "\"\"\"\n",
        "  - Logistic Regression\n",
        "  - Lemmatization\n",
        "  - Bag of Word\n",
        "  - Min_df = 0\n",
        "\"\"\"\n",
        "\n",
        "print(\"Training :\")\n",
        "testing_model(model,x_train_bow1,y_train)\n",
        "print(\"Testing : \")\n",
        "testing_model(model,x_test_bow1,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training :\n",
            "F1 Score : 0.800756190248842\n",
            "Acc. : 0.7957767934381097\n",
            "Testing : \n",
            "F1 Score : 0.7858920628312001\n",
            "Acc. : 0.7791897404163693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tykTo7xaZTuc",
        "outputId": "ef20c8c6-942e-402a-a5af-7df9a482ce95"
      },
      "source": [
        "model = pickle.load(open('/content/Logistic_Regression_tfidf_lem.pkl','rb'))\n",
        "\n",
        "\"\"\"\n",
        "  - Logistic Regression\n",
        "  - Lemmatization\n",
        "  - TF-IDF\n",
        "  - Min_df = 0\n",
        "\"\"\"\n",
        "\n",
        "print(\"Training :\")\n",
        "testing_model(model,x_train_tfidf1,y_train)\n",
        "print(\"Testing : \")\n",
        "testing_model(model,x_test_tfidf1,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training :\n",
            "F1 Score : 0.8015236925348527\n",
            "Acc. : 0.7979915576912148\n",
            "Testing : \n",
            "F1 Score : 0.7856056202149633\n",
            "Acc. : 0.7810935220908132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13v-AsKJbb9b"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9XpN7TrbhHn"
      },
      "source": [
        "## Bernoulli"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvrQAnKhbXpQ",
        "outputId": "aee56839-a8c5-4630-a32d-5885a0b67a69"
      },
      "source": [
        "model = pickle.load(open('/content/Bernoulli_NB_bow_lem.pkl','rb'))\n",
        "\n",
        "\"\"\"\n",
        "  - Bernoulli Naive Bayes\n",
        "  - Lemmatization\n",
        "  - Bag of Word\n",
        "  - Min_df = 0\n",
        "\"\"\"\n",
        "\n",
        "print(\"Training :\")\n",
        "testing_model(model,x_train_bow1,y_train)\n",
        "print(\"Testing : \")\n",
        "testing_model(model,x_test_bow1,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training :\n",
            "F1 Score : 0.8051521595475144\n",
            "Acc. : 0.8035742474853033\n",
            "Testing : \n",
            "F1 Score : 0.7733481723732724\n",
            "Acc. : 0.7698160624869766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmN3Ib0-bk98",
        "outputId": "4f56936d-7bd1-4690-c8c2-8e5e8a58417b"
      },
      "source": [
        "model = pickle.load(open('/content/Bernoulli_NB_tfidf_lem.pkl','rb'))\n",
        "\n",
        "\"\"\"\n",
        "  - Bernoulli Naive Bayes\n",
        "  - Lemmatization\n",
        "  - TF-IDF\n",
        "  - Min_df = 0\n",
        "\"\"\"\n",
        "\n",
        "print(\"Training :\")\n",
        "testing_model(model,x_train_tfidf1,y_train)\n",
        "print(\"Testing : \")\n",
        "testing_model(model,x_test_tfidf1,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training :\n",
            "F1 Score : 0.8051521595475144\n",
            "Acc. : 0.8035742474853033\n",
            "Testing : \n",
            "F1 Score : 0.7733481723732724\n",
            "Acc. : 0.7698160624869766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1grKKIjblbH"
      },
      "source": [
        "## Multinomial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FN_xV5Kbkpx",
        "outputId": "047859e7-3ff7-4b73-f1c6-1fdeeddf8edb"
      },
      "source": [
        "model = pickle.load(open('/content/MultinomialNB_bow_lem.pkl','rb'))\n",
        "\n",
        "\"\"\"\n",
        "  - Multinomial Naive Bayes\n",
        "  - Lemmatization\n",
        "  - Bag of Word\n",
        "  - Min_df = 0\n",
        "\"\"\"\n",
        "\n",
        "print(\"Training :\")\n",
        "testing_model(model,x_train_bow1,y_train)\n",
        "print(\"Testing : \")\n",
        "testing_model(model,x_test_bow1,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training :\n",
            "F1 Score : 0.8019615071343096\n",
            "Acc. : 0.8036776452462288\n",
            "Testing : \n",
            "F1 Score : 0.7679696471737737\n",
            "Acc. : 0.7690772815386849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so54TIOSbno-",
        "outputId": "baa554e4-10da-440f-d1a8-9f67c6590017"
      },
      "source": [
        "model = pickle.load(open('/content/MultinomialNB_tfidf_lem.pkl','rb'))\n",
        "\n",
        "\"\"\"\n",
        "  - Multinomial Naive Bayes\n",
        "  - Lemmatization\n",
        "  - TF-IDF\n",
        "  - Min_df = 0\n",
        "\"\"\"\n",
        "\n",
        "print(\"Training :\")\n",
        "testing_model(model,x_train_tfidf1,y_train)\n",
        "print(\"Testing : \")\n",
        "testing_model(model,x_test_tfidf1,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training :\n",
            "F1 Score : 0.8030909260426536\n",
            "Acc. : 0.8049641975386597\n",
            "Testing : \n",
            "F1 Score : 0.7617398487640592\n",
            "Acc. : 0.7632428063573048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfSeqFeeboGo"
      },
      "source": [
        "# Gradient Boosting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UISbVPuzbsap",
        "outputId": "8f306eab-bf5b-4743-8fee-58c3c31aa06f"
      },
      "source": [
        "model = pickle.load(open('/content/GBC_Classifier_tfidf_lem2.pkl','rb'))\n",
        "\n",
        "\"\"\"\n",
        "  - Gradient Boosting\n",
        "  - Lemmatization\n",
        "  - TF-IDF\n",
        "  - Min_df = 5\n",
        "  - lr = 1.5\n",
        "  - n = 150\n",
        "  - depth = 10\n",
        "\"\"\"\n",
        "\n",
        "print(\"Training :\")\n",
        "testing_model(model,x_train_tfidf,y_train)\n",
        "print(\"Testing : \")\n",
        "testing_model(model,x_test_tfidf,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training :\n",
            "F1 Score : 0.8065127112261151\n",
            "Acc. : 0.8002773585739633\n",
            "Testing : \n",
            "F1 Score : 0.7779714526018113\n",
            "Acc. : 0.7699676072968826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eQDCr_veGA5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}